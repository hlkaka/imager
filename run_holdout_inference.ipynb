{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1738750",
   "metadata": {},
   "source": [
    "# Runs inference on the holdout set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d8e81d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.CTDataSet import CTDicomSlices, DatasetManager\n",
    "from data.CustomTransforms import Window, Imagify, Normalize\n",
    "\n",
    "from models.UNet_L import UNet\n",
    "\n",
    "import albumentations as A\n",
    "from torchvision import transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch\n",
    "\n",
    "from torchmetrics.classification import BinaryConfusionMatrix\n",
    "from pytorch_lightning import Trainer, loggers as pl_loggers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2b311c",
   "metadata": {},
   "source": [
    "## Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3083d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_set = \"/root/checkpoints/holdout_set.txt\"  # may need full path to this file\n",
    "dataset = '/root/imager/organized_dataset_2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28fd350",
   "metadata": {},
   "source": [
    "### Image preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49ecb38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# important constants\n",
    "\n",
    "WL = 50\n",
    "WW = 200\n",
    "\n",
    "img_size = 256\n",
    "\n",
    "mean = 61.0249\n",
    "std = 78.3195\n",
    "\n",
    "batch_size = 32\n",
    "num_workers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43e562ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "prep = transforms.Compose([Window(WL, WW), Imagify(WL, WW), Normalize(mean, std)])\n",
    "\n",
    "resize_tsfm = A.Compose([A.Resize(img_size, img_size)],\n",
    "            additional_targets={\"image1\": 'image', \"mask1\": 'mask'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec0a3d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsm = DatasetManager.load_train_val_test(dataset, holdout_set, holdout_set, holdout_set) # train, val and test will be identical. we will use test\n",
    "\n",
    "_, _, test_dicoms = dsm.get_dicoms() # DICOM glob is preset in the class file\n",
    "\n",
    "test_ds = CTDicomSlices(test_dicoms, preprocessing = prep, resize_transform = resize_tsfm, n_surrounding=1)\n",
    "\n",
    "datasets = {}\n",
    "datasets['train'] = None\n",
    "datasets['val'] = None\n",
    "datasets['test'] = test_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0859e714",
   "metadata": {},
   "source": [
    "## Get models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d2a4c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetTester(UNet):\n",
    "    \"\"\" Same as UNet but with a tester method that tracks more metrics. This allows us to avoid editing the original code. \"\"\"\n",
    "    def __init__(self, datasets, backbone :str = 'resnet34', encoder_weights :str = 'imagenet',\n",
    "                 classes :int = 2, activation :str = 'softmax', batch_size :int = 32,\n",
    "                 lr = 0.0001, dl_workers = 8, optimizer_params = None, in_channels=3,\n",
    "                 loss = 'dice'):\n",
    "        super().__init__(datasets, backbone = backbone, encoder_weights=encoder_weights, classes=classes,\n",
    "                activation=activation, batch_size=batch_size, lr=lr, dl_workers=dl_workers,\n",
    "                optimizer_params=optimizer_params, in_channels=in_channels, loss=loss)\n",
    "        self.bcm = BinaryConfusionMatrix()\n",
    "        \n",
    "    def test_step(self, batch, batch_nb):\n",
    "        images, masks, _, _ = batch\n",
    "\n",
    "        y_hat = self(images)\n",
    "\n",
    "        # loss dim is [batch, 1, img_x, img_y]\n",
    "        # need to get rid of the second dimension so\n",
    "        # size matches with mask\n",
    "        loss = self.loss(y_hat, masks)\n",
    "\n",
    "        # binary_classification_metrics\n",
    "        ground_truth = torch.amax(masks, (1, 2)) \n",
    "        \n",
    "        preds = torch.amax(y_hat[:, 0, :, :], (1, 2))   # y_hat dim is (batch, 2, img_sz, img_sz). 2 for 2 classes. We only need the first. To make dimension match ground truth masks\n",
    "\n",
    "        conf_matrix = self.bcm(preds, ground_truth)\n",
    "\n",
    "        # Logs\n",
    "        #tensorboard_logs = {'val_loss': loss}\n",
    "        return {'test_loss': loss, \"tp\": conf_matrix[0, 0], \"fn\": conf_matrix[0, 1], \"fp\": conf_matrix[1, 0], \"tn\": conf_matrix[1, 1]} #, 'log': tensorboard_logs}\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        test_loss_mean = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "        self.log('test_loss_mean', test_loss_mean, logger=True)\n",
    "\n",
    "        tp = torch.stack([x['tp'] for x in outputs]).sum()\n",
    "        fn = torch.stack([x['fn'] for x in outputs]).sum()\n",
    "        fp = torch.stack([x['fp'] for x in outputs]).sum()\n",
    "        tn = torch.stack([x['tn'] for x in outputs]).sum()\n",
    "\n",
    "        self.log('tp', tp, logger=True)\n",
    "        self.log('fn', fn, logger=True)\n",
    "        self.log('fp', fp, logger=True)\n",
    "        self.log('tn', tn, logger=True)\n",
    "\n",
    "        return {'test_loss': test_loss_mean, \"tp\": tp, \"fn\": fn, \"fp\": fp, \"tn\": tn}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416a1fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_imagenet = \"/root/checkpoints/imagenet.ckpt\"\n",
    "ckpt_random = \"/root/checkpoints/random_nopretrain.ckpt\"\n",
    "ckpt_jigsaw = \"/root/checkpoints/ckpt_jigsaw_classic.ckpt\"\n",
    "ckpt_felz = \"/root/checkpoints/felz.ckpt\"\n",
    "ckpt_jigsaw_sr = \"/root/checkpoints/jigsaw_sr.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "486c1e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(path):\n",
    "    return UNetTester.load_from_checkpoint(path, datasets=datasets, in_channels=3, classes=2)\n",
    "\n",
    "def get_model_dir(ckpt):\n",
    "    return ckpt[0:ckpt.find('/logs/default')]\n",
    "\n",
    "# imagenet, random, felz, jigsaw, jigsawSR\n",
    "\n",
    "#model_imagenet = get_model(ckpt_imagenet)\n",
    "#model_random = get_model(ckpt_random)\n",
    "#model_jigsaw = get_model(ckpt_jigsaw)\n",
    "#model_felz = get_model(ckpt_felz)\n",
    "#model_jigsaw_sr = get_model(ckpt_jigsaw_sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294a3036",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:441: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
      "  rank_zero_deprecation(\n",
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_lightning/plugins/precision/native_amp.py:56: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27f28f5dad4744c3b5d115596723b86b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1751: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Runningstage.testing metric      DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "           fn                      296.0\n",
      "           fp                      432.0\n",
      "     test_loss_mean         0.22868618369102478\n",
      "           tn                     1718.0\n",
      "           tp                     8710.0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:233: UserWarning: You called `self.log('tp', ...)` in your `test_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
      "  warning_cache.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:233: UserWarning: You called `self.log('fn', ...)` in your `test_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
      "  warning_cache.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:233: UserWarning: You called `self.log('fp', ...)` in your `test_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
      "  warning_cache.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:233: UserWarning: You called `self.log('tn', ...)` in your `test_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
      "  warning_cache.warn(\n"
     ]
    }
   ],
   "source": [
    "# felz_seg was trained using a different dataset split without a holdout. make sure dataset is set to\n",
    "# felz before running felz_seg\n",
    "model_ckpts = [ckpt_imagenet, ckpt_random, ckpt_jigsaw, ckpt_felz, ckpt_jigsaw_sr]\n",
    "ckpt_labels = [\"imagenet\", \"random\", \"jigsaw_classic\", \"felz_seg\", \"jigsaw_sr\"]\n",
    "\n",
    "for m_ckpt, m_label in zip(model_ckpts, ckpt_labels):\n",
    "    model = get_model(m_ckpt)\n",
    "    model_dir = '/root/test_outputs/'\n",
    "    \n",
    "    tb_logger = pl_loggers.TensorBoardLogger('{}-test-{}'.format(model_dir, m_label))\n",
    "\n",
    "    trainer = Trainer(gpus=1, accelerator='gpu', precision=16, logger = tb_logger, default_root_dir=model_dir)\n",
    "\n",
    "    trainer.test(model = model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
